# -*- coding: utf-8 -*-
"""Flujo completo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F2fEPG9WYFq3DNZwVwf9ZiRNaDnsc8rF
"""

#!pip install tableone > NULL

# For processing data
from sklearn.preprocessing import OneHotEncoder, StandardScaler
import pandas as pd
from sklearn.model_selection import train_test_split
from scipy import stats
import numpy as np
import re

#from tableone import TableOne

## To modeling
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier


# Assesment
from sklearn.metrics import classification_report

"""# Funciones limpieza y preprocesamiento"""

def percentage_nulls(df,nan_str):
    """
    This function returns a dictionary with the column and
    the porcentage of missing values

    nan_str: array con los valores nulos no reconocidos (ej. nan, NA, null) identificados
    """
    df = df.replace(nan_str,np.nan,regex=True)

    N_rows = df.shape[0]
    percentage_vars = {}
    for var in df.columns:
        percentage_vars[var]=(df[var].isnull().sum() / N_rows)
    return percentage_vars

#bloque para clasificar variables
def binary(data):
  bin_reg = r"^[01](?:\.0)?\.?$"
  return bool(re.findall(bin_reg, str(data)))

def flotante(data):
  float_reg = r"\b\d+(?:\.)?\d*\b"
  return bool(re.findall(float_reg, str(data)))

def vars_type(base, umbral):
  binarias = [] #se guardan primero las binarias y se agregan las string al final
  numericas = []
  cat_str = []
  for columna in base.columns:
    if base[columna].apply(binary).sum()/len(base[columna]) > umbral:
      binarias.append(columna)
    elif base[columna].apply(flotante).sum()/len(base[columna]) > umbral:
      numericas.append(columna)
    else:
      cat_str.append(columna)
  #return pd.DataFrame({"cat_bin": categoricas, "num":numericas, 'cat_str':cat_str})
  #print('cat binarias, cat string, númericas')
  return binarias, cat_str, numericas

def normalidad(df,numericas):
    nonormal = []
    normal = []
    for columna in df[numericas]:
        if columna in numericas:
                n,p = stats.shapiro(df[columna])
                if p<0.05:
                    nonormal.append(columna)
                else:
                    normal.append(columna)
    return nonormal, normal


def remove_nan(df_, percentages_dict, threshold, nonormal, normal):
    """
    Receive a dictionary with the percatege of missing of each varaible and drop them
    according to the threshold defined
    """
    df = df_.copy()
    for var in percentages_dict:
        if percentages_dict[var] > threshold:
            df.drop(columns = [var], inplace=True)
        elif var in nonormal:
          mediana = df[var].median()
          df = df.replace(np.nan,mediana) #variable no normal pon mediana
        elif var in normal:
          media = df[var].mean()
          df = df.replace(np.nan,media) #variable normal pon la media
    return df

def std_scaler(nums, df_):
    """
    standardizing nums(array con nombres de numerical) variables
    con standard scaler
    """
    df = df_.copy()
    scaler = StandardScaler()
    df_scaled = pd.DataFrame(scaler.fit_transform(df[nums]), columns=nums)
    df.drop(columns=nums, inplace=True)
    df = pd.concat([df,df_scaled], axis=1)
    return df

def dummies_ohe(df_,cats):
    """
    Returns a dataframe with dummies,and dropped the categorical in original
    the cats arguments receive the cats to transform.
    """
    df = df_.copy()
    ohe = OneHotEncoder(drop='first',handle_unknown='ignore', sparse_output=False)
    dummies = pd.DataFrame(ohe.fit_transform(df[cats]))
    dummies.columns = ohe.get_feature_names_out()  #Names ohe.get_feature_names_out()-> all dummies
    df.drop(columns=cats, inplace=True)
    df = pd.concat([df,dummies], axis=1)
    return df


def split_df(df,target):
    """
    Split the data in X,y to ML implementations
    """
    X = df.loc[ : , df.columns != target]
    y = df[target]

    print(f'next: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = True, random_state = 666, stratify=y)')
    return X,y

"""# Procesamiento"""

#usarla nos ayuda a ver desde antes (del análisis) si las variables son informativas o no
#copiar y pegar
'''
TableOne(df,
         nonnormal = nonormal,
         categorical=cat_str,
         groupby='', #por cual se quiere agrupar
         pval=True,
         htest_name=True)
'''

## Decision tree grid
def grid_dt(X_train, y_train):
    model = DecisionTreeClassifier(random_state=1000)
    class_weight =  [{0:0.05, 1:0.95}, {0:0.1, 1:0.9}, {0:0.2, 1:0.8}, {0:0.5, 1:0.5}]
    max_depth = None,
    min_samples_leaf = [5, 10, 20, 50, ]
    criterion  = ["gini", "entropy"]
    grid = dict(class_weight=class_weight, max_depth=max_depth, min_samples_leaf=min_samples_leaf, criterion=criterion)
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv,
                           scoring='accuracy',error_score='raise')
    grid_result = grid_search.fit(X_train, y_train)
    return grid_search.best_estimator_

def best_pred_dt(X_train,y_train, X_test):
  best_model = grid_dt(X_train, y_train) #entreno el modelo con X_train y y_train
  preds_dt = best_model.predict(X_test)
  return best_model,preds_dt

# Grid search hyperparameters for a logistic regression model
def grid_search_lr(X_train, y_train):
    model = LogisticRegression(random_state=666, max_iter=1000)
    class_weight =  [{0:0.05, 1:0.95}, {0:0.1, 1:0.9}, {0:0.2, 1:0.8}]
    solvers = ['liblinear']
    penalty = ['l2','l1']
    c_values = [ 10, 1.0, 0.1, 0.01, 0.001, ]
    grid = dict(solver=solvers,penalty=penalty,C=c_values, class_weight= class_weight)
    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)
    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv,
                           scoring='f1',error_score=0)
    grid_result = grid_search.fit(X_train, y_train)
    best_params = grid_result.best_params_
    return grid_result.best_estimator_

# best_model = grid_search_lr(X_train, y_train) #entreno el modelo con X_train y y_train
# preds_lr = best_model.predict(X_test)

## MLP grid perceptrón
def grid_MLP(X_train, y_train):
    model = MLPClassifier(random_state=1,
                          max_iter=100)
    hidden_layer_sizes = [(8,), (100,), (3,3,16,) ,(5,5,5,) ]
    activation =  ['tanh', 'relu', 'logistic']
    solver =  ['sgd', 'adam']
    alpha  = [0.0001, 0.05]
    learning_rate = ['constant','adaptive']
    grid = dict(hidden_layer_sizes=hidden_layer_sizes, activation= activation, solver= solver, learning_rate=learning_rate, alpha=alpha)
    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)
    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv,
                           scoring='f1',error_score='raise')
    grid_result = grid_search.fit(X_train, y_train)
    return  grid_result.best_estimator_

# best_model = grid_MLP(X_train, y_train)
# preds_MLP = best_model.predict(X_test)

# print(classification_report(y_test, preds_dt))  # recall igual a sensibilidad
# print(classification_report(y_test, preds_MLP))
# print(classification_report(y_test, preds_lr))

"""#Aplicación con diabetes.csv

# PRE
"""

url = "https://raw.githubusercontent.com/4GeeksAcademy/decision-tree-project-tutorial/main/diabetes.csv"
df = pd.read_csv(url)

df

df.describe()

(df['BloodPressure'] == 0).value_counts()

(df['Insulin']== 0).value_counts()

(df['Glucose'] == 0).value_counts() #posible NAN

df['Glucose'] = df['Glucose'].replace(0,np.nan) #prueba para funcion remove nan

(df['Glucose'] == 0).value_counts()

valores = []
for columna in df.columns:
  valores.append(df[columna].unique())

nan_str = ['NAN']
porcent_nulos = percentage_nulls(df,nan_str)
print(porcent_nulos)

binarias, categoricas, numericas = vars_type(df, 0.8)
print(binarias, categoricas, numericas)

nonormal, normal = normalidad(df,numericas)
print(nonormal, normal)

df = remove_nan(df, porcent_nulos, 0.65, nonormal, normal)

df_scaled = std_scaler(numericas, df)
df_scaled

#df = dummies_ohe(df_scaled,categoricas) #si hubiera categóricas, aquí se convertirían

''''
TableOne(df,
         nonnormal = nonormal,
         categorical=categoricas,
         groupby='Outcome', #por cual se quiere agrupar
         pval=True,
         htest_name=True)
'''

"""#Procesamiento"""

X,y = split_df(df,'Outcome')

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = True, random_state = 666, stratify=y) #modificar si se quiere

#Decision tree
grid_dt(X_train, y_train)

best_model, preds_dt = best_pred_dt(X_train,y_train, X_test)

#LR
best_lr = grid_search_lr(X_train, y_train)
best_lr

preds_lr = best_lr.predict(X_test)

best_MLP = grid_MLP(X_train, y_train)

best_MLP

preds_MLP = best_MLP.predict(X_test)

print(f'Decision tree: \n {classification_report(y_test, preds_dt)}')
print(f'Logistic regression:\n {classification_report(y_test, preds_lr)}')
print(f'MLP: \n {classification_report(y_test, preds_MLP)}')

import pickle
with open("Modelo_ArbolD.pkl", 'wb') as j:
    pickle.dump("best_dt", j)